---
title: Questions of Interest
layout: default
last_modified_at: 2023-08-18
---

# Questions of Interest from Summer 2023

This month, I graduated from Georgia Tech.
I now must decide what I want to do next.

Motivated by the impact of my work, I know that I want to focus on addressing emergent risks of AI systems.
I have a great opportunity to define my own role entering the nascent field of AI safety.
However, as a generalist in skills and interests, I have become a bit overwhelmed by the number of options.

To help myself, and to help others help me, I have compiled a list of the topics I have been thinking about this summer. They roughly correspond to what I wish to work on in the future, but also represent the context of my current projects at the AI Safety Initiative at Georgia Tech.

### Questions

- Can we create a cross-university AI safety research institute similar to the NBER?
- How can academia and industry collaborate on alignment research?
- Which alignment problems can we expect for-profit companies solve?
- Which institutions can we rely on to conduct effective and comprehensive model evaluations?
- How can we create a knowledge-sharing network for AI safety given some information may be hazardous?
- Which fields have strong frameworks for guiding the development of dual-use technology?
- What disincentives exist for professors to work on AI safety?
- What simple bits of information, if understood by policymakers, would best illustrate the magnitude of AI risk?
- What types of alignment problems can be addressed through competitions and prizes?
- On what dimensions should we define AI risks?
- How can we accommodate various cultural perspectives on AI safety?
- Can we design a CAPTCHA alternative that collects data for alignment research?
- Can we train language models to learn values embedded in human text?
- At what point of specification does a normative question turn into an empirical one?
- Which factors of intelligence enable systems to answer which normative questions?
